# Classificador de E-mails AutoU üöÄ

Este projeto √© uma solu√ß√£o pr√°tica para classifica√ß√£o autom√°tica de e-mails e sugest√£o de respostas, utilizando **Intelig√™ncia Artificial open-source** (Ollama com o modelo `mistral:instruct`) e t√©cnicas de NLP. Ele foi desenvolvido como parte do desafio da AutoU e est√° pronto para deploy, uso local, apresenta√ß√£o e avalia√ß√£o.

---

## ‚ú® Funcionalidades

- **Upload de e-mails** em `.txt` ou `.pdf` ou colagem direta do texto.
- **Classifica√ß√£o autom√°tica** em _Produtivo_ ou _Improdutivo_ usando IA.
- **Resposta autom√°tica gerada pela IA** em portugu√™s, cordial e pronta para envio ao cliente.
- **Pr√©-processamento NLP** com NLTK: stemming, stopwords etc.
- **Interface web responsiva** (HTML, CSS, JS), modo claro/escuro, UX amig√°vel.
- **Backend Python/Flask** simples e robusto.
- **Deploy-ready** para nuvem (Railway, Heroku, Render, etc.)

---

## üì∏ Demonstra√ß√£o (Workflow)

1. **Acesse a interface web**.
2. Fa√ßa upload de um `.txt` ou `.pdf` de e-mail **ou cole o texto**.
3. Clique em **Enviar**.
4. Veja:
   - **Categoria:** _Produtivo_ ou _Improdutivo_
   - **Resposta sugerida:** Pronta para copiar/usar
   - **Pr√©via do texto processado**

---

## üöÄ Como rodar localmente

### 1. Requisitos

- **Python 3.9+**
- **[Ollama](https://ollama.com/download) instalado e rodando localmente**
- Modelo `mistral:instruct` baixado no Ollama:
  ```sh
  ollama pull mistral:instruct
  ollama serve
  ```
- Depend√™ncias Python:
  ```
  pip install -r backend/requirements.txt
  ```
  (inclui `flask`, `requests`, `flask-cors`, `pdfplumber`, `nltk`)

### 2. Estrutura de Pastas

```
backend/
  requirements.txt
  app.py
  .env
  src/
    classifier.py
    responder.py
    utils.py
  tests/
   test_classifier.py
   test_responder.py
    
frontend/
  index.html
  main.js
  style.css
README.md
```

### 3. Configura√ß√£o do arquivo `.env`

O backend utiliza vari√°veis de ambiente para configurar a integra√ß√£o com o Ollama.  
**Crie um arquivo chamado `.env` na raiz do projeto (ou dentro da pasta `backend/` se desejar) com o seguinte conte√∫do:**

```env
OLLAMA_URL=http://localhost:11434/api/generate
OLLAMA_MODEL=mistral:instruct
```

- `OLLAMA_URL`: Endere√ßo local da API do Ollama (padr√£o para instala√ß√£o local).
- `OLLAMA_MODEL`: Nome do modelo baixado no Ollama.

Se quiser usar outro modelo ou um servidor Ollama remoto, basta alterar esses valores no `.env`, sem necessidade de alterar o c√≥digo.

### 4. Rodando o backend

No diret√≥rio `backend/`:

```sh
python app.py

or

python backend/app.py

```

O backend Flask rodar√° por padr√£o em `http://localhost:5000/`.

### 5. Rodando o frontend

Acesse `http://localhost:5000/` no navegador.

---

## ‚òÅÔ∏è Deploy na nuvem

Voc√™ pode fazer deploy facilmente em [Railway](https://railway.app/), [Render](https://render.com/), [Heroku](https://heroku.com/) etc.

- **Importante:** O Ollama precisa estar rodando em um servidor com suporte a containers ou VMs. Em ambientes gratuitos, pode ser necess√°rio adaptar para usar uma API de LLM externa.
- O frontend pode ser hospedado em Vercel, Netlify, ou na pr√≥pria pasta `frontend` do Flask.

---

## üß† Decis√µes t√©cnicas

- **Modelo IA:** `mistral:instruct` via [Ollama](https://ollama.com/). Leve, open-source e com desempenho adequado para classifica√ß√£o bin√°ria e respostas em portugu√™s.
- **NLP:** NLTK para pr√©-processamento (stopwords, stemming), facilitando futuras expans√µes.
- **Prompt engineering:** Prompts expl√≠citos e robustos, evitando respostas amb√≠guas da IA.
- **Fallbacks:** Resposta padr√£o amig√°vel caso a IA falhe.

---

## üìù Como testar

- Fa√ßa upload de um arquivo `.txt` ou `.pdf` com um e-mail, ou cole o texto no campo correspondente.
- Clique em **Enviar**.
- Veja a classifica√ß√£o e a resposta sugerida.

---

## üì¶ Depend√™ncias

```
flask
flask-cors
werkzeug
pdfplumber
nltk
requests
python-dotenv
pytest
```

---

## üß™ Testes automatizados com pytest

O projeto j√° vem com testes automatizados utilizando o [pytest](https://docs.pytest.org/) para garantir a qualidade e o funcionamento dos principais m√≥dulos do backend.

### Como rodar os testes

1. Certifique-se de que as depend√™ncias est√£o instaladas:
   ```
   pip install -r backend/requirements.txt
   ```

2. Navegue at√© a pasta do backend:
   ```
   cd backend
   ```

3. Execute o pytest:
   ```
   pytest
   ```

   Voc√™ ver√° uma sa√≠da parecida com:
   ```
   collected 3 items

   tests/test_classifier.py ..                                                                                      [ 66%]
   tests/test_responder.py .                                                                                        [100%]

   ================================================== 3 passed in 0.87s ==================================================
   ```

   - Os pontos `.` indicam testes que passaram.
   - O progresso (`[ 66%]`, `[100%]`) mostra o andamento da execu√ß√£o.
   - Se todos os testes passaram, est√° tudo certo!

### Estrutura dos testes

- Os testes est√£o no diret√≥rio `backend/tests/`.
- Cada arquivo testa um m√≥dulo espec√≠fico, por exemplo:
  - `test_classifier.py` testa a classifica√ß√£o de e-mails.
  - `test_responder.py` testa a limpeza e prepara√ß√£o de respostas autom√°ticas.

### Observa√ß√µes sobre depend√™ncias externas

- Os testes de classifica√ß√£o dependem do Ollama estar rodando e do modelo carregado. Para garantir testes consistentes e independentes, recomenda-se utilizar mocks nas fun√ß√µes que fazem chamadas externas (exemplo no pr√≥prio c√≥digo de teste).
- Se algum teste falhar, verifique se o Ollama est√° ativo e o modelo correto est√° dispon√≠vel, ou utilize mocks conforme exemplo comentado nos arquivos de teste.

### Mais informa√ß√µes

- Para saber mais sobre pytest, consulte [a documenta√ß√£o oficial](https://docs.pytest.org/).

---

## üìπ V√≠deo Demonstrativo

Confira o v√≠deo demonstrativo:

- [Youtube - 1](https://www.youtube.com/watch?v=QMO2L-pq_X4&ab_channel=KauanVidigal)
- [Youtube - 2](https://www.youtube.com/watch?v=bMJZEQ9ocEU)

---

## üóÇÔ∏è Exemplos de arquivos `.env`

```env
# Exemplo de .env para rodar localmente
OLLAMA_URL=http://localhost:11434/api/generate
OLLAMA_MODEL=mistral:instruct
```

> **Dica:** Nunca suba o arquivo `.env` para reposit√≥rios p√∫blicos!

---

## ü§ù Contato

D√∫vidas, feedbacks ou sugest√µes?  
Entre em contato: **[Kauan Vidigal]** ‚Ä¢ [kauanvidigalcontato@gmail.com]

---

## üéâ Observa√ß√µes finais

- O projeto est√° pronto para ser testado por usu√°rios leigos e t√©cnicos.
- O c√≥digo √© limpo, modular e f√°cil de adaptar.
- Sinta-se √† vontade para sugerir melhorias ou expandir o projeto!

---

> **AutoU Case Pr√°tico ‚Äî Solu√ß√£o por [Kauan Vidigal] - Github [https://github.com/Vidigal-code]**