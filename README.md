# Classificador de E-mails AutoU üöÄ

Este projeto √© uma solu√ß√£o pr√°tica para classifica√ß√£o autom√°tica de e-mails e sugest√£o de respostas, utilizando **Intelig√™ncia Artificial open-source** (Ollama com o modelo `mistral:instruct`) e t√©cnicas de NLP. Ele foi desenvolvido como parte do desafio da AutoU e est√° pronto para deploy, uso local, apresenta√ß√£o e avalia√ß√£o.

---

## ‚ú® Funcionalidades

- **Upload de e-mails** em `.txt` ou `.pdf` ou colagem direta do texto.
- **Classifica√ß√£o autom√°tica** em _Produtivo_ ou _Improdutivo_ usando IA.
- **Resposta autom√°tica gerada pela IA** em portugu√™s, cordial e pronta para envio ao cliente.
- **Pr√©-processamento NLP** com NLTK: stemming, stopwords etc.
- **Interface web responsiva** (HTML, CSS, JS), modo claro/escuro, UX amig√°vel.
- **Backend Python/Flask** simples e robusto.
- **Deploy-ready** para nuvem (Railway, Heroku, Render, etc.)

---

## üì∏ Demonstra√ß√£o (Workflow)

1. **Acesse a interface web**.
2. Fa√ßa upload de um `.txt` ou `.pdf` de e-mail **ou cole o texto**.
3. Clique em **Enviar**.
4. Veja:
   - **Categoria:** _Produtivo_ ou _Improdutivo_
   - **Resposta sugerida:** Pronta para copiar/usar
   - **Pr√©via do texto processado**

---

## üöÄ Como rodar localmente

### 1. Requisitos

- **Python 3.9+**
- **[Ollama](https://ollama.com/download) instalado e rodando localmente**
- Modelo `mistral:instruct` baixado no Ollama:
  ```sh
  ollama pull mistral:instruct
  ollama serve
  ```
- Depend√™ncias Python:
  ```
  pip install -r backend/requirements.txt
  ```
  (inclui `flask`, `requests`, `flask-cors`, `pdfplumber`, `nltk`)

### 2. Estrutura de Pastas

```
backend/
   requirements.txt
  app.py
  src/
    classifier.py
    responder.py
    utils.py
    
frontend/
  index.html
  main.js
  style.css
README.md
```

### 3. Rodando o backend

No diret√≥rio `backend/`:

```sh
python app.py

or

python backend/app.py

```

O backend Flask rodar√° por padr√£o em `http://localhost:5000/`.

### 4. Rodando o frontend

Acesse `http://localhost:5000/` no navegador.

---

## ‚òÅÔ∏è Deploy na nuvem

Voc√™ pode fazer deploy facilmente em [Railway](https://railway.app/), [Render](https://render.com/), [Heroku](https://heroku.com/) etc.

- **Importante:** O Ollama precisa estar rodando em um servidor com suporte a containers ou VMs. Em ambientes gratuitos, pode ser necess√°rio adaptar para usar uma API de LLM externa.
- O frontend pode ser hospedado em Vercel, Netlify, ou na pr√≥pria pasta `frontend` do Flask.

---

## üß† Decis√µes t√©cnicas

- **Modelo IA:** `mistral:instruct` via [Ollama](https://ollama.com/). Leve, open-source e com desempenho adequado para classifica√ß√£o bin√°ria e respostas em portugu√™s.
- **NLP:** NLTK para pr√©-processamento (stopwords, stemming), facilitando futuras expans√µes.
- **Prompt engineering:** Prompts expl√≠citos e robustos, evitando respostas amb√≠guas da IA.
- **Fallbacks:** Resposta padr√£o amig√°vel caso a IA falhe.

---

## üìù Como testar

- Fa√ßa upload de um arquivo `.txt` ou `.pdf` com um e-mail, ou cole o texto no campo correspondente.
- Clique em **Enviar**.
- Veja a classifica√ß√£o e a resposta sugerida.

---

## üì¶ Depend√™ncias

```
flask
flask-cors
werkzeug
pdfplumber
nltk
requests
```

---
## üìπ V√≠deo Demonstrativo

Confira o v√≠deo demonstrativo:

- [Youtube](https://www.youtube.com/watch?v=QMO2L-pq_X4&ab_channel=KauanVidigal)



## ü§ù Contato

D√∫vidas, feedbacks ou sugest√µes?  
Entre em contato: **[Kauan Vidigal]** ‚Ä¢ [kauanvidigalcontato@gmail.com]

---

## üéâ Observa√ß√µes finais

- O projeto est√° pronto para ser testado por usu√°rios leigos e t√©cnicos.
- O c√≥digo √© limpo, modular e f√°cil de adaptar.
- Sinta-se √† vontade para sugerir melhorias ou expandir o projeto!

---

> **AutoU Case Pr√°tico ‚Äî Solu√ß√£o por [Kauan Vidigal] - Github [https://github.com/Vidigal-code]**
